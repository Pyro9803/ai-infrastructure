---
# Default variables for tinyllama_deploy role
# These can be overridden in group_vars or host_vars

# Environment (set via group_vars or extra vars)
# Environment (set via group_vars or extra vars)
tinyllama_deploy_environment: dev
tinyllama_deploy_terraform_env: "{{ tinyllama_deploy_environment }}"

# Paths
tinyllama_deploy_project_root: "{{ playbook_dir }}/../.."
tinyllama_deploy_terraform_dir: "{{ tinyllama_deploy_project_root }}/terraform/envs/{{ tinyllama_deploy_terraform_env }}"
tinyllama_deploy_deployment_dir: "{{ tinyllama_deploy_project_root }}/ray/tinyllama"

# Kubernetes
tinyllama_deploy_kubernetes:
  namespace: ai
  deployment_name: tinyllama-service
  config_map_name: vllm-app-code

# KubeRay operator configuration
tinyllama_deploy_kuberay:
  operator_version: "1.4.2"
  install_timeout: "5m"
  ready_timeout: 300

# Timeouts (in seconds)
tinyllama_deploy_timeouts:
  pod_ready: 1200  # 20 minutes for large image pulls
  deployment_ready: 1800
  service_ready: 600
  serve_ready: 900

# Model configuration
tinyllama_deploy_model:
  name: TinyLlama/TinyLlama-1.1B-Chat-v1.0
  replica_count: 1
  test_prompt: "Hello! What can you help me with?"
  test_max_tokens: 50

# Resource requirements
tinyllama_deploy_resources:
  gpu_required: true
  min_gpu_nodes: 1

# Testing and verification
tinyllama_deploy_testing:
  enabled: true
  test_endpoint: true
  run_health_checks: true

# Monitoring and logging
tinyllama_deploy_monitoring:
  enabled: false
  log_level: INFO

# Deployment strategy
tinyllama_deploy_deployment:
  strategy: rolling  # replace or rolling
  cleanup_on_failure: true
  wait_for_ready: true
